{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "victorian-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import sklearn.model_selection\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vertical-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 45000\n",
    "embedding_dim = 512\n",
    "max_length = 100\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "biblical-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('classifier_data_0.csv', header = 0, encoding= 'utf-8')\n",
    "dataset.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6','Unnamed: 7','Unnamed: 8','Unnamed: 9', 'Unnamed: 10'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "innovative-sudan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            description  \\\n",
      "4576  product version       see aboutversionurls if ...   \n",
      "3457  Version: 32.0.1653.0 (Official Build 225591) c...   \n",
      "3879  When I try to add my self to review a code in ...   \n",
      "788   Currently enter_chroot.sh has a hack to preser...   \n",
      "307   Product Version      : <see about:version>URLs...   \n",
      "\n",
      "                        owner  \n",
      "4576                      NaN  \n",
      "3457      kinaba@chromium.org  \n",
      "3879    nsylvain@chromium.org  \n",
      "788   davidjames@chromium.org  \n",
      "307          jon@chromium.org  \n"
     ]
    }
   ],
   "source": [
    "X= dataset['description']\n",
    "y = dataset['owner']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1500)\n",
    "X_train.shape\n",
    "df = pd.DataFrame(data=[X_train, y_train], index=[\"description\", \"owner\"]).T\n",
    "df = df.append(pd.DataFrame(data=[X_test, y_test], index=[\"description\", \"owner\"]).T)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "handy-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['char_count'] = df['description'].apply(len) # Number of characters in the string\n",
    "df['word_count'] = df['description'].apply(lambda x: len(x.split())) # Number of words in the string \n",
    "df['word_density'] = df['char_count'] / (df['word_count']+1) # Density of word (in char)\n",
    "df['title_word_count'] = df['description'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "committed-textbook",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_selection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-45798ae0931e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'owner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'owner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_selection' is not defined"
     ]
    }
   ],
   "source": [
    "train_x, valid_x, y_train, y_valid = model_selection.train_test_split(df['description'], df['owner'], random_state=42, stratify=df['owner'], test_size=0.2)\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['description'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=300)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=300)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "def create_conv_model(word_index, label, embedding_matrix, pre_trained=False):\n",
    "    '''\n",
    "    Function to generate a convulational neural network for binary or multiclass classification.\n",
    "    @param word_index: (matrix) unique token in corpus\n",
    "    @param label: (list) list of labels to determine if it,s a binary or multiclass\n",
    "    @param embedding_matrix: (matrix) matrix of integer for each word in the \n",
    "    @param pre_trained: (bool) determine if the model will use pretrained model\n",
    "    @return: (model) convulational neural network \n",
    "    '''\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) +1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Conv1D(100, 5, activation='relu'), # padding='same'\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Conv1D(32, 5, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.GlobalMaxPooling1D(),\n",
    "\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_conv_model(word_index, labels, pre_trained=pre_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    " history = model.fit(train_seq_x, train_y,\n",
    "                    epochs=50, callbacks=[es],\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(valid_seq_x, valid_y)\n",
    "\n",
    "    print(results)\n",
    "    print(f\"\\nThe precision of the model is {round(100*precision_score(valid_y, (model.predict(valid_seq_x)>0.5).astype(int), labels),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-perfume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-setup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-impact",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-sustainability",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-saturday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-polls",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-helmet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-investigation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv] *",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
